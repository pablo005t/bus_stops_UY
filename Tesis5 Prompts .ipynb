{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7f6daccc965d4d02ae72eedf5c469f0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c94c64db40a4e5d9830aecefe446e89","IPY_MODEL_81294bce0070444f8f6aea1bce538413","IPY_MODEL_8821de58559e4e75ab61a44126a9a67a"],"layout":"IPY_MODEL_ae20020b415441dbaa555cd4dee45b3d"}},"8c94c64db40a4e5d9830aecefe446e89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7294aeef37b644ea83d94e82d9b6f3a8","placeholder":"​","style":"IPY_MODEL_d0a8316a3da34a4c990108fffd5fe5cc","value":"Loading checkpoint shards: 100%"}},"81294bce0070444f8f6aea1bce538413":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d2d1e9ae2be420882152d96276274ef","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7903e881bac5494cad64f5b259800ef9","value":5}},"8821de58559e4e75ab61a44126a9a67a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afee1d2bda924b41bcc30a77610c3b26","placeholder":"​","style":"IPY_MODEL_32eee7d0906442ce9ed7b3a887ef8c42","value":" 5/5 [01:25&lt;00:00, 14.68s/it]"}},"ae20020b415441dbaa555cd4dee45b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7294aeef37b644ea83d94e82d9b6f3a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a8316a3da34a4c990108fffd5fe5cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d2d1e9ae2be420882152d96276274ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7903e881bac5494cad64f5b259800ef9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afee1d2bda924b41bcc30a77610c3b26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32eee7d0906442ce9ed7b3a887ef8c42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39e960e112c447958cf521cfded3d3a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d55e916bd70d47a2b803c6227646ff52","IPY_MODEL_6e920ac841784758ad53dec33c017c50","IPY_MODEL_04bf7639c0794db7a4330ddd92251afd"],"layout":"IPY_MODEL_3f2ee1de492d4aac95203f1d9b94d9f0"}},"d55e916bd70d47a2b803c6227646ff52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c31144f141044078afc07f5f2ec2f8c","placeholder":"​","style":"IPY_MODEL_80d445af2522413cb44d0478aa25dca0","value":"Loading checkpoint shards: 100%"}},"6e920ac841784758ad53dec33c017c50":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdb073120c3c49f7aadb63e4984f7c7d","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9233022d40404dafa12a6b370c107545","value":5}},"04bf7639c0794db7a4330ddd92251afd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f3f27f5c4f04e318ff30228878c3d50","placeholder":"​","style":"IPY_MODEL_bda899e211364495861866500bc4a2a8","value":" 5/5 [00:18&lt;00:00, 18.81s/it]"}},"3f2ee1de492d4aac95203f1d9b94d9f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c31144f141044078afc07f5f2ec2f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80d445af2522413cb44d0478aa25dca0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdb073120c3c49f7aadb63e4984f7c7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9233022d40404dafa12a6b370c107545":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f3f27f5c4f04e318ff30228878c3d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bda899e211364495861866500bc4a2a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"52a36ab3"},"source":["#%pip install bitsandbytes"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["7f6daccc965d4d02ae72eedf5c469f0f","8c94c64db40a4e5d9830aecefe446e89","81294bce0070444f8f6aea1bce538413","8821de58559e4e75ab61a44126a9a67a","ae20020b415441dbaa555cd4dee45b3d","7294aeef37b644ea83d94e82d9b6f3a8","d0a8316a3da34a4c990108fffd5fe5cc","6d2d1e9ae2be420882152d96276274ef","7903e881bac5494cad64f5b259800ef9","afee1d2bda924b41bcc30a77610c3b26","32eee7d0906442ce9ed7b3a887ef8c42"]},"id":"ce3kvVqjPbKV","outputId":"105fad87-18a3-4d36-8e7e-05bafdb5e51b","executionInfo":{"status":"ok","timestamp":1755215602561,"user_tz":180,"elapsed":110615,"user":{"displayName":"Pablo Maurente","userId":"10671774458804850149"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n","You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py:2199: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6daccc965d4d02ae72eedf5c469f0f"}},"metadata":{}}],"source":["from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n","import torch\n","torch.backends.cuda.matmul.allow_tf32 = True  # T4 acelera un poco\n","\n","mname = \"Qwen/Qwen2-VL-7B-Instruct\"\n","bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","                         bnb_4bit_compute_dtype=torch.bfloat16)\n","\n","proc = AutoProcessor.from_pretrained(mname)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    mname,\n","    quantization_config=bnb,\n","    device_map=\"auto\",              # reparte GPU/CPU si hace falta\n","    torch_dtype=torch.bfloat16\n",")\n","\n","# generación estable (sin sampling) y salida corta\n","gen_kwargs = dict(max_new_tokens=128, temperature=0.0)\n"]},{"cell_type":"code","source":["from PIL import Image\n","import json"],"metadata":{"id":"cq5b3H-BpfR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PROMPT = (\n","    \"You are verifying annotations made by a computer vision model that detects bus stops in satellite imagery. \"\n","    \"Each image contains a marked area predicted as a possible bus stop. Your task is to confirm or reject the prediction \"\n","    \"based only on clear visual evidence.\\n\\n\"\n","\n","    \"Instructions:\\n\"\n","    \"- Confirm (true) only if you clearly see a bus stop, such as a shelter or a labeled stop sign.\\n\"\n","    \"- Reject (false) if it shows a tree, pole, billboard, vehicle, house entrance, or any non-stop object.\\n\"\n","    \"- If unsure or unclear, default to false.\\n\\n\"\n","\n","    \"Respond ONLY in this JSON format:\\n\"\n","    '{ \"is_bus_stop\": true|false, \"reason\": \"short one-line explanation\" }\\n\\n'\n","\n","    \"Examples:\\n\"\n","    '{ \"is_bus_stop\": false, \"reason\": \"billboard near road, not a bus stop\" }\\n'\n","    '{ \"is_bus_stop\": true, \"reason\": \"clearly labeled bus stop sign\" }'\n",")\n","\n","\n"],"metadata":{"id":"QnBXy7QMdtjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_bus_stops(img_path):\n","    img = Image.open(img_path).convert(\"RGB\")\n","    messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"Act as a strict annotation verifier assessing the correctness of bus stop predictions in satellite imagery.\"\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            { \"type\": \"image\", \"image\": img },\n","            { \"type\": \"text\", \"text\": PROMPT }\n","        ]\n","    }\n","    ]\n","    # 1. Preparar texto del chat\n","    chat_text = proc.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n","    # 2. Convertir texto+imagen a tensores\n","    inputs = proc(text=chat_text, images=img, return_tensors=\"pt\").to(model.device)\n","    # 3. Generar (greedy decoding)\n","    out = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n","    # 4. Decodificar y extraer JSON\n","    txt = proc.decode(out[0], skip_special_tokens=True)\n","    #jtxt = txt[txt.find(\"{\"): txt.rfind(\"}\")+1]\n","    return txt\n","\n"],"metadata":{"id":"DTdjNfpnqEVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res = detect_bus_stops(\"/content/ruta8_pt_11602_z20.png\")\n","print(res)"],"metadata":{"id":"fyQULmJUrEj7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755215618100,"user_tz":180,"elapsed":15452,"user":{"displayName":"Pablo Maurente","userId":"10671774458804850149"}},"outputId":"27a76184-0073-4e22-bdbe-f8b303e653b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["system\n","Act as a strict annotation verifier assessing the correctness of bus stop predictions in satellite imagery.\n","user\n","You are verifying annotations made by a computer vision model that detects bus stops in satellite imagery. Each image contains a marked area predicted as a possible bus stop. Your task is to confirm or reject the prediction based only on clear visual evidence.\n","\n","Instructions:\n","- Confirm (true) only if you clearly see a bus stop, such as a shelter or a labeled stop sign.\n","- Reject (false) if it shows a tree, pole, billboard, vehicle, house entrance, or any non-stop object.\n","- If unsure or unclear, default to false.\n","\n","Respond ONLY in this JSON format:\n","{ \"is_bus_stop\": true|false, \"reason\": \"short one-line explanation\" }\n","\n","Examples:\n","{ \"is_bus_stop\": false, \"reason\": \"billboard near road, not a bus stop\" }\n","{ \"is_bus_stop\": true, \"reason\": \"clearly labeled bus stop sign\" }\n","assistant\n","{\n","  \"is_bus_stop\": false,\n","  \"reason\": \"no clear visual evidence of a bus stop\"\n","}\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import json\n","import re # Import the re module\n","\n","PROMPT = (\n","  \"Eres un verificador. La imagen fue MARCADA por otro modelo como posible PARADA DE ÓMNIBUS. \"\n","  \"Puede estar mal. Si NO estás seguro, responde false.\\n\"\n","  \"Responde SOLO JSON válido, sin texto extra:\\n\"\n","  \"{\\\"is_bus_stop\\\": true|false, \\\"reason\\\": \\\"≤1 línea\\\"}\\n\"\n","  \"Criterios: aceptar solo si se ve claramente un refugio o una señal de parada junto a una carretera; \"\n","  \"rechazar postes, árboles, carteles, vehículos, casetas/galpones, entradas de casas.\"\n",")\n","\n","def safe_json(txt):\n","    m = re.search(r\"\\{.*\\}\", txt, re.S)\n","    if not m:\n","        return {\"is_bus_stop\": False, \"reason\": \"no JSON\"}\n","    try:\n","        return json.loads(m.group())\n","    except json.JSONDecodeError:\n","        return {\"is_bus_stop\": False, \"reason\": \"invalid JSON\"}\n","\n","def tile_iter(img, size=896, overlap=128):\n","    W,H = img.size\n","    step = size - overlap\n","    xs = list(range(0, max(1, W-size)+1, step)) or [0]\n","    ys = list(range(0, max(1, H-size)+1, step)) or [0]\n","    for y in ys:\n","        for x in xs:\n","            yield img.crop((x,y,x+size,y+size)), (x,y,x+size,y+size)\n","\n","def qwen_verify(img, fewshot_pos, fewshot_neg):\n","    \"\"\"few-shot visual: 1 positivo + 1 negativo antes de la consulta\"\"\"\n","    msgs = [\n","        {\"role\":\"system\",\"content\":\"Responde estrictamente en JSON válido.\"},\n","\n","        # Ejemplo POSITIVO\n","        {\"role\":\"user\",\"content\":[\n","            {\"type\":\"image\",\"image\": fewshot_pos},\n","            {\"type\":\"text\",\"text\": PROMPT}\n","        ]},\n","        {\"role\":\"assistant\",\"content\":\n","            \"{\\\"is_bus_stop\\\": true, \\\"reason\\\": \\\"refugio pequeño junto a la ruta\\\"}\"},\n","\n","        # Ejemplo NEGATIVO\n","        {\"role\":\"user\",\"content\":[\n","            {\"type\":\"image\",\"image\": fewshot_neg},\n","            {\"type\":\"text\",\"text\": PROMPT}\n","        ]},\n","        {\"role\":\"assistant\",\"content\":\n","            \"{\\\"is_bus_stop\\\": false, \\\"reason\\\": \\\"caseta/cabina en campo, no parada\\\"}\"}\n","    ]\n","    return msgs\n","\n","def classify_image_with_tiles(path_img, path_pos_example, path_neg_example):\n","    img = Image.open(path_img).convert(\"RGB\")\n","    pos = Image.open(path_pos_example).convert(\"RGB\")\n","    neg = Image.open(path_neg_example).convert(\"RGB\")\n","\n","    best = {\"is_bus_stop\": False, \"reason\": \"no tile accepted\", \"tile\": None}\n","\n","    for tile, bbox in tile_iter(img, size=896, overlap=128):\n","        msgs = qwen_verify(tile, pos, neg)\n","        msgs.append({\"role\":\"user\",\"content\":[\n","            {\"type\":\"image\",\"image\": tile},\n","            {\"type\":\"text\",\"text\": PROMPT}\n","        ]})\n","        chat = proc.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n","        inputs = proc(text=chat, images=tile, return_tensors=\"pt\").to(model.device)\n","        print(\"Inputs before model generate:\", inputs) # Added print statement\n","        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n","        ans = safe_json(proc.decode(out[0], skip_special_tokens=True))\n","\n","        # siempre sobreescribe con el último tile válido\n","        if ans.get(\"is_bus_stop\") == True or best[\"is_bus_stop\"] is False:\n","            best = {**ans, \"tile\": bbox}\n","\n","    return best\n","\n","# Ejemplo:\n","# result = classify_image_with_tiles(\"data/img.png\", \"fewshot/pos.jpg\", \"fewshot/neg.jpg\")\n","# print(result)"],"metadata":{"id":"UXA8hieQ_Vs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = classify_image_with_tiles(\"/content/ruta8_pt_11932_z20.png\", \"/content/ruta8_pt_11602_z20.png\", \"/content/ruta8_pt_11845_z20.png\")\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"Al9ZGPqe_sKB","executionInfo":{"status":"error","timestamp":1755215792443,"user_tz":180,"elapsed":143,"user":{"displayName":"Pablo Maurente","userId":"10671774458804850149"}},"outputId":"e8b40e3b-a3ea-4571-ea3a-0d60f896d03c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 1 is out of bounds for dimension 0 with size 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2269881796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_image_with_tiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/ruta8_pt_11932_z20.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/ruta8_pt_11602_z20.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/ruta8_pt_11845_z20.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3706501331.py\u001b[0m in \u001b[0;36mclassify_image_with_tiles\u001b[0;34m(path_img, path_pos_example, path_neg_example)\u001b[0m\n\u001b[1;32m     67\u001b[0m         ]})\n\u001b[1;32m     68\u001b[0m         \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inputs before model generate:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Added print statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mnum_image_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_grid_thw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mmerge_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                     \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<|placeholder|>\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_image_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_VEqOM6FdY9","executionInfo":{"status":"ok","timestamp":1755368138598,"user_tz":180,"elapsed":21400,"user":{"displayName":"Ana Araújo","userId":"14657118052263951842"}},"outputId":"58b45c61-7830-458a-d76d-d469997ff46b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Pipeline de verificación de paradas (versión 1)\n","================================================\n","\n","Objetivo\n","--------\n","Dado un conjunto de **imágenes satelitales con la predicción de Roboflow dibujada** (rectángulo rojo\n","marcando la candidata) y/o (opcional) un archivo con metadatos (bbox, lat/lon del centro del tile),\n","esté pipeline decide si cada candidata es realmente una **parada de ómnibus**.\n","\n","Qué hace este pipeline\n","----------------------\n","1) **Detección del recorte (crop) de la candidata** a partir del rectángulo rojo pintado en la imagen de Roboflow.\n","   - Si también tenés un CSV/JSON con bbox numéricas, puede usarse directamente (más robusto).\n","2) **Verificación con un VLM** (por ejemplo Qwen/Qwen2-VL-7B-Instruct) usando **dos vistas**:\n","   - *FULL*: el mosaico completo con la caja dibujada (contexto).\n","   - *CROP*: un recorte ampliado (~30% de padding) sobre la caja (detalle).\n","   - Respuesta **JSON estricta**: `{is_bus_stop, reason, confidence}`.\n","3) **Robustez por consenso** con **rotaciones del recorte** (0/90/180/270):\n","   - Majority vote y promedio de confianza; si hay dudas, se marca para revisión.\n","4) **Gate geo-espacial (opcional)**:\n","   - Si se dispone de lat/lon del centro del tile y del zoom, se convierte el **pixel center del bbox a lat/lon**\n","     y se calcula distancia al eje de la carretera. Se rechaza si está lejos (p.ej. >8 m).\n","   - (Opcional) Gate por intersección con edificaciones para descartar techos/galpones.\n","5) **Clasificador ligero (opcional)** con embeddings **SigLIP/CLIP + LogisticRegression** sobre crops.\n","   - Se puede entrenar rápidamente con positivos/negativos duros.\n","6) **Salida consolidada** en CSV: decisión final, razón, confianzas parciales, flags de gates.\n","\n","Modificaciones incluidas vs. versión anterior\n","---------------------------------------------\n","- **Nuevo prompt** específico con criterios visuales medibles y política \"falso por defecto\".\n","- **Doble entrada al VLM** (FULL + CROP) en lugar de solo una imagen.\n","- **Parser JSON estricto** con saneo de respuestas para evitar texto extra.\n","- **Consenso por rotaciones** del CROP con voto mayoritario y umbrales de confianza.\n","- **Gate geo-espacial** opcional: distancia a carretera y (opcional) edificios.\n","- **Soporte de dos fuentes de bbox**: (a) detectadas desde el rectángulo rojo pintado, (b) leídas desde CSV/JSON.\n","- **Módulo opcional SigLIP/CLIP** para verificación rápida basada en embeddings.\n","- **Registro a CSV** con trazabilidad (razones, confianzas, flags, errores).\n","- **Post-hoc recalibración** y **bucketing** (true/false/review) sin re-inferir el modelo.\n","- **Galería HTML** opcional para validación visual.\n","- **Debug de crops**: exporta pares FULL+BOX y CROP para inspección.\n","\n","Cómo usar\n","---------\n","1) Ajustá las rutas en la sección CONFIG.\n","2) Si tenés metadatos (CSV/JSON) con columnas: `filename, x, y, width, height, center_lat, center_lon, zoom`,\n","   poné la ruta en `PATH_BBOX_META` y `USE_META_BBOX=True`.\n","3) Si **no** tenés bbox meta, el código intentará **detectar el rectángulo rojo** en la imagen.\n","4) (Opcional) Si disponés de **shapefile/GeoJSON de carreteras** para el gate, configurá `PATH_ROADS`.\n","5) (Opcional) Si querés **clasificador SigLIP**, prepará un CSV de entrenamiento con\n","   `filename,label` (1=parada,0=no) y apuntalo en `PATH_CLF_TRAIN`.\n","\n","Notas\n","-----\n","- El gate geo-espacial requiere conocer la geolocalización: ya sea vía (a) CSV meta por imagen, o (b) parseo del\n","  nombre del archivo (ver regex `FILENAME_LATLON_REGEX`).\n","- Si no hay geo, el pipeline sigue funcionando (salta ese gate).\n","- Para Qwen2-VL, reutilizá tu `model` y `processor` existentes; este archivo sólo define las llamadas.\n","\n","\"\"\"\n","\n","import os\n","import re\n","import json\n","import math\n","import csv\n","from dataclasses import dataclass\n","from typing import Optional, Tuple, List, Dict, Any\n","\n","import numpy as np\n","from PIL import Image, ImageDraw\n","\n","# Dependencias opcionales (se usan si están instaladas)\n","try:\n","    import geopandas as gpd\n","    from shapely.geometry import Point\n","except Exception:\n","    gpd = None\n","    Point = None\n","\n","try:\n","    from sklearn.linear_model import LogisticRegression\n","    import joblib\n","except Exception:\n","    LogisticRegression = None\n","    joblib = None\n","\n","try:\n","    from transformers import AutoProcessor, AutoModel\n","    import torch\n","except Exception:\n","    AutoProcessor = None\n","    AutoModel = None\n","    torch = None\n","\n","try:\n","    import pandas as pd\n","except Exception:\n","    pd = None\n","\n","\n","# ======================\n","# CONFIG\n","# ======================\n","\n","IMAGES_DIR = \"/content/drive/My Drive/Tesis/Datos/Ruta8/paradas_no_seguras\"  # carpeta con imágenes predichas\n","OUTPUT_CSV = \"/content/drive/My Drive/Tesis/Resultados/verificacion_paradas_v1.csv\"\n","\n","# Metadatos opcionales con bbox y/o geo\n","USE_META_BBOX = False\n","PATH_BBOX_META = \"/content/drive/My Drive/Tesis/Datos/meta_bbox.csv\"  # columns: filename,x,y,width,height[,center_lat,center_lon,zoom]\n","\n","# Gate geo (opcional)\n","USE_GEO_GATE = False\n","PATH_ROADS = \"/content/drive/My Drive/Tesis/Datos/rutas_uy.geojson\"  # eje de ruta 8 (o nacional)\n","MAX_DIST_TO_ROAD_M = 8.0\n","\n","# Buildings (opcional)\n","USE_BUILDINGS_GATE = False\n","PATH_BUILDINGS = \"/content/drive/My Drive/Tesis/Datos/buildings_uy.geojson\"\n","\n","# Intento de parsear lat/lon desde el nombre del archivo\n","FILENAME_LATLON_REGEX = re.compile(r\"lat_(-?\\d+\\.\\d+)_lon_(-?\\d+\\.\\d+)\")\n","DEFAULT_ZOOM = 20\n","TILE_SIZE = (640, 640)  # ancho, alto\n","\n","# Consenso por rotaciones (solo CROP)\n","ROTATIONS = [0, 90, 180, 270]\n","VOTE_THR = 0.6     # promedio de confianza para aceptar\n","UNCERTAIN_RANGE = (0.35, 0.6)\n","\n","# Prompt para el VLM\n","PROMPT_VLM = (\n","    \"You are a strict verifier of bus-stop predictions on top-down satellite imagery of Uruguay.\\n\\n\"\n","    \"You will receive TWO images of the same location:\\n\"\n","    \"1) FULL tile with a thin red rectangle marking the candidate.\\n\"\n","    \"2) CROP: a zoom-in of the marked rectangle with ~20–40 m across.\\n\\n\"\n","    \"Decide if the marked candidate is a BUS STOP.\\n\\n\"\n","    \"Positive cues (all usually present): small shelter (≈2–4 m wide) with a short roof shadow next to the road edge,\\n\"\n","    \"or a simple sign pole with small shadow right at the road edge. Aligned roughly parallel to the road and within ~0–5 m from the edge.\\n\\n\"\n","    \"Negative cues: houses/roofs (>6 m), sheds, truck cabins, containers, billboards, median road signs, trees/bushes,\\n\"\n","    \"objects far from the road edge or inside lots, or blurry views. If unsure, reject.\\n\\n\"\n","    \"Output ONLY JSON: {\\\"is_bus_stop\\\": true|false, \\\"reason\\\": \\\"short one-line\\\", \\\"confidence\\\": 0.00-1.00}.\\n\"\n","    \"No extra text.\"\n",")\n","\n","PROMPT_VLM_CROP_ONLY = (\n","    \"You are a strict verifier of a bus-stop candidate on top-down satellite imagery.\\n\"\n","    \"You will receive a single zoomed-in crop around the candidate.\\n\"\n","    \"Accept only if a small shelter (≈2–4 m) or a bus-stop sign pole is clearly visible near the road edge.\\n\"\n","    \"If unsure, reject. Output ONLY JSON: {\\\"is_bus_stop\\\": true|false, \\\"reason\\\": \\\"short\\\", \\\"confidence\\\": 0.00-1.00}.\"\n",")\n","\n","\n","# ======================\n","# Utils: bbox desde el rectángulo rojo dibujado\n","# ======================\n","\n","def detect_red_box_bbox(img: Image.Image, r_thr: int = 170, gb_thr: int = 120) -> Optional[Tuple[int, int, int, int]]:\n","    \"\"\"Detecta el rectángulo rojo dibujado (estilo overlay de Roboflow) y retorna bbox (x_center,y_center,w,h) en px.\n","    Estrategia simple: umbral por color, tomar el bbox de todos los píxeles rojos.\n","    Si no se detecta, retorna None.\n","    \"\"\"\n","    arr = np.array(img.convert(\"RGB\"))\n","    R, G, B = arr[..., 0], arr[..., 1], arr[..., 2]\n","    mask = (R >= r_thr) & (G <= gb_thr) & (B <= gb_thr)\n","\n","    ys, xs = np.where(mask)\n","    if ys.size < 20:  # muy pocos pixeles rojos\n","        return None\n","    x0, x1 = int(xs.min()), int(xs.max())\n","    y0, y1 = int(ys.min()), int(ys.max())\n","\n","    # a veces el overlay incluye texto rojo: si el bbox es demasiado fino, puede ser ruido.\n","    if (x1 - x0) < 8 or (y1 - y0) < 8:\n","        return None\n","\n","    # Convertir a (cx, cy, w, h)\n","    w = x1 - x0 + 1\n","    h = y1 - y0 + 1\n","    cx = x0 + w // 2\n","    cy = y0 + h // 2\n","    return (cx, cy, w, h)\n","\n","\n","def crop_from_bbox(img: Image.Image, bbox_xywh: Tuple[int, int, int, int], pad: float = 0.3) -> Image.Image:\n","    W, H = img.size\n","    cx, cy, w, h = bbox_xywh\n","    pw, ph = int(w * pad), int(h * pad)\n","    x0 = max(0, cx - w // 2 - pw)\n","    y0 = max(0, cy - h // 2 - ph)\n","    x1 = min(W, cx + w // 2 + pw)\n","    y1 = min(H, cy + h // 2 + ph)\n","    return img.crop((x0, y0, x1, y1))\n","\n","\n","# ======================\n","# Utils: geo (Web Mercator helpers)\n","# ======================\n","\n","def _latlon_to_meters(lat: float, lon: float) -> Tuple[float, float]:\n","    origin_shift = 2 * math.pi * 6378137 / 2.0\n","    mx = lon * origin_shift / 180.0\n","    my = math.log(math.tan((90 + lat) * math.pi / 360.0)) * 6378137\n","    return mx, my\n","\n","\n","def _meters_to_latlon(mx: float, my: float) -> Tuple[float, float]:\n","    lon = (mx / (2 * math.pi * 6378137 / 2.0)) * 180.0\n","    lat = (2 * math.atan(math.exp(my / 6378137)) - math.pi / 2) * 180.0 / math.pi\n","    return lat, lon\n","\n","\n","def pixel_to_latlon(center_lat: float, center_lon: float, zoom: int, tile_size: Tuple[int, int],\n","                    px: float, py: float) -> Tuple[float, float]:\n","    \"\"\"Convierte coordenada de pixel dentro del tile (0..W, 0..H) a lat/lon, dada la lat/lon del centro del tile.\n","    Aproximación Web Mercator.\n","    \"\"\"\n","    W, H = tile_size\n","    res0 = 156543.03392804097  # m/px at zoom 0\n","    res = res0 / (2 ** zoom)\n","\n","    mx0, my0 = _latlon_to_meters(center_lat, center_lon)\n","    dx = (px - W / 2) * res\n","    dy = (py - H / 2) * res\n","    lat, lon = _meters_to_latlon(mx0 + dx, my0 - dy)  # ojo con el eje y (top-down)\n","    return lat, lon\n","\n","\n","# ======================\n","# VLM: prompts y parsing JSON\n","# ======================\n","\n","def parse_json_strict(txt: str) -> Dict[str, Any]:\n","    m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n","    if not m:\n","        return {\"is_bus_stop\": False, \"reason\": \"no JSON\", \"confidence\": 0.0}\n","    raw = m.group(0)\n","    raw = raw.replace(\"True\", \"true\").replace(\"False\", \"false\")\n","    raw = re.sub(r\",\\s*}\\s*$\", \"}\", raw)\n","    raw = re.sub(r\",\\s*]\", \"]\", raw)\n","    try:\n","        obj = json.loads(raw)\n","    except Exception:\n","        return {\"is_bus_stop\": False, \"reason\": \"JSON parse error\", \"confidence\": 0.0}\n","    ibs = bool(obj.get(\"is_bus_stop\", False))\n","    reason = str(obj.get(\"reason\", \"\"))[:160]\n","    try:\n","        conf = float(obj.get(\"confidence\", 0.0))\n","    except Exception:\n","        conf = 0.0\n","    conf = max(0.0, min(1.0, conf))\n","    return {\"is_bus_stop\": ibs, \"reason\": reason, \"confidence\": conf}\n","\n","\n","def vlm_verify_full_and_crop(full_img: Image.Image, crop_img: Image.Image,\n","                             model, processor, prompt: str = PROMPT_VLM) -> Dict[str, Any]:\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Act as a strict annotation verifier for bus stops on top-down satellite imagery. Be conservative.\"},\n","        {\"role\": \"user\", \"content\": [\n","            {\"type\": \"text\", \"text\": prompt},\n","            {\"type\": \"image\", \"image\": full_img},\n","            {\"type\": \"image\", \"image\": crop_img},\n","        ]}\n","    ]\n","    chat_text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n","    # Pass batch lists: text=[...], images=[[...]] to satisfy processor batching\n","    inputs = processor(text=[chat_text], images=[[full_img, crop_img]], return_tensors=\"pt\", padding=True).to(model.device)\n","    out = model.generate(**inputs, max_new_tokens=128, do_sample=False, temperature=0.0, top_p=1.0)\n","    txt = processor.decode(out[0], skip_special_tokens=True)\n","    return parse_json_strict(txt)\n","\n","\n","def vlm_verify_crop_only(crop_img: Image.Image, model, processor, prompt: str = PROMPT_VLM_CROP_ONLY) -> Dict[str, Any]:\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Act as a strict annotation verifier for bus stops on top-down satellite imagery. Be conservative.\"},\n","        {\"role\": \"user\", \"content\": [\n","            {\"type\": \"text\", \"text\": prompt},\n","            {\"type\": \"image\", \"image\": crop_img},\n","        ]}\n","    ]\n","    chat_text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n","    # Batch lists to avoid processor quirks across versions\n","    inputs = processor(text=[chat_text], images=[[crop_img]], return_tensors=\"pt\", padding=True).to(model.device)\n","    out = model.generate(**inputs, max_new_tokens=128, do_sample=False, temperature=0.0, top_p=1.0)\n","    txt = processor.decode(out[0], skip_special_tokens=True)\n","    return parse_json_strict(txt)\n","\n","\n","# ======================\n","# SigLIP/CLIP (opcional)\n","# ======================\n","\n","def load_siglip_model(device: str = \"cuda\"):\n","    if AutoProcessor is None or AutoModel is None:\n","        raise RuntimeError(\"Transformers no disponible. Instala: pip install transformers accelerate\")\n","    proc = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n","    mdl = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\")\n","    if torch is not None:\n","        mdl = mdl.to(device)\n","        mdl.eval()\n","    return mdl, proc\n","\n","\n","def siglip_embed(img: Image.Image, mdl, proc, device: str = \"cuda\") -> np.ndarray:\n","    if torch is None:\n","        raise RuntimeError(\"PyTorch no disponible\")\n","    inputs = proc(images=img, return_tensors=\"pt\")\n","    if device:\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","    with torch.no_grad():\n","        feats = mdl.get_image_features(**inputs)\n","    v = feats[0].detach().cpu().numpy()\n","    v = v / (np.linalg.norm(v) + 1e-9)\n","    return v\n","\n","\n","def train_light_classifier(train_csv: str, images_dir: str, device: str = \"cuda\",\n","                           save_path: Optional[str] = None) -> Any:\n","    if LogisticRegression is None:\n","        raise RuntimeError(\"scikit-learn no disponible. Instala: pip install scikit-learn joblib\")\n","    mdl, proc = load_siglip_model(device)\n","    X, y = [], []\n","    with open(train_csv, newline='', encoding='utf-8') as f:\n","        reader = csv.DictReader(f)\n","        for row in reader:\n","            fn = row['filename']\n","            label = int(row['label'])\n","            p = os.path.join(images_dir, fn)\n","            if not os.path.isfile(p):\n","                continue\n","            img = Image.open(p).convert('RGB')\n","            emb = siglip_embed(img, mdl, proc, device)\n","            X.append(emb)\n","            y.append(label)\n","    X = np.stack(X, axis=0)\n","    y = np.array(y)\n","    clf = LogisticRegression(max_iter=300, class_weight='balanced')\n","    clf.fit(X, y)\n","    if save_path and joblib is not None:\n","        joblib.dump({\"clf\": clf}, save_path)\n","    return clf\n","\n","\n","def load_light_classifier(path: str):\n","    if joblib is None:\n","        raise RuntimeError(\"joblib no disponible. Instala: pip install joblib\")\n","    obj = joblib.load(path)\n","    return obj[\"clf\"]\n","\n","\n","def clf_score_crop(crop_img: Image.Image, clf, siglip_mdl, siglip_proc, device: str = \"cuda\") -> float:\n","    emb = siglip_embed(crop_img, siglip_mdl, siglip_proc, device)\n","    p = float(clf.predict_proba(emb.reshape(1, -1))[0, 1])\n","    return p\n","\n","\n","# ======================\n","# Gate geo\n","# ======================\n","\n","def load_roads(path: str):\n","    if gpd is None:\n","        raise RuntimeError(\"GeoPandas no disponible. Instala: pip install geopandas shapely pyproj\")\n","    roads = gpd.read_file(path)\n","    roads = roads.to_crs(3857)\n","    roads[\"_geom_len_m\"] = roads.geometry.length\n","    return roads\n","\n","\n","def load_buildings(path: str):\n","    if gpd is None:\n","        raise RuntimeError(\"GeoPandas no disponible\")\n","    gdf = gpd.read_file(path).to_crs(3857)\n","    return gdf\n","\n","\n","def dist_to_nearest_road_m(lat: float, lon: float, roads_gdf) -> float:\n","    if gpd is None:\n","        return float('inf')\n","    mx, my = _latlon_to_meters(lat, lon)\n","    p = Point(mx, my)\n","    # índice espacial si existe\n","    try:\n","        idx = roads_gdf.sindex\n","        cand_idx = list(idx.nearest(p.bounds, num_results=8))\n","        d = min(roads_gdf.iloc[i].geometry.distance(p) for i in cand_idx)\n","    except Exception:\n","        d = roads_gdf.distance(p).min()\n","    return float(d)\n","\n","\n","def intersects_building(lat: float, lon: float, buildings_gdf) -> bool:\n","    if gpd is None:\n","        return False\n","    mx, my = _latlon_to_meters(lat, lon)\n","    p = Point(mx, my)\n","    try:\n","        idx = buildings_gdf.sindex\n","        cand_idx = list(idx.intersection(p.buffer(3).bounds))  # 3 m buffer\n","        return any(buildings_gdf.iloc[i].geometry.buffer(0).intersects(p.buffer(1.5)) for i in cand_idx)\n","    except Exception:\n","        return bool(buildings_gdf.intersects(p.buffer(1.5)).any())\n","\n","\n","# ======================\n","# Meta utils\n","# ======================\n","\n","@dataclass\n","class ImageMeta:\n","    filename: str\n","    bbox_xywh: Optional[Tuple[int, int, int, int]] = None  # px\n","    center_lat: Optional[float] = None\n","    center_lon: Optional[float] = None\n","    zoom: Optional[int] = None\n","\n","\n","def read_meta_csv(path: str) -> Dict[str, ImageMeta]:\n","    meta: Dict[str, ImageMeta] = {}\n","    with open(path, newline='', encoding='utf-8') as f:\n","        reader = csv.DictReader(f)\n","        for row in reader:\n","            fn = row['filename']\n","            im = ImageMeta(filename=fn)\n","            try:\n","                x = int(float(row.get('x', '')))\n","                y = int(float(row.get('y', '')))\n","                w = int(float(row.get('width', '')))\n","                h = int(float(row.get('height', '')))\n","                im.bbox_xywh = (x, y, w, h)\n","            except Exception:\n","                pass\n","            try:\n","                im.center_lat = float(row.get('center_lat', ''))\n","                im.center_lon = float(row.get('center_lon', ''))\n","            except Exception:\n","                pass\n","            try:\n","                im.zoom = int(row.get('zoom', ''))\n","            except Exception:\n","                im.zoom = None\n","            meta[fn] = im\n","    return meta\n","\n","\n","def parse_latlon_from_name(name: str) -> Tuple[Optional[float], Optional[float]]:\n","    m = FILENAME_LATLON_REGEX.search(name)\n","    if not m:\n","        return None, None\n","    return float(m.group(1)), float(m.group(2))\n","\n","\n","# ======================\n","# Pipeline principal\n","# ======================\n","\n","def verify_image(\n","    img_path: str,\n","    model,\n","    processor,\n","    roads_gdf=None,\n","    buildings_gdf=None,\n","    meta: Optional[ImageMeta] = None,\n","    clf=None,\n","    siglip_mdl=None,\n","    siglip_proc=None,\n","    device: str = \"cuda\",\n",") -> Dict[str, Any]:\n","    \"\"\"Procesa una imagen y devuelve un dict con el resultado consolidado.\"\"\"\n","    out: Dict[str, Any] = {\n","        \"filename\": os.path.basename(img_path),\n","        \"status\": \"ok\",\n","        \"has_bbox\": False,\n","        \"geo_gate_used\": False,\n","        \"geo_ok\": True,\n","        \"dist_to_road_m\": None,\n","        \"building_intersection\": None,\n","        \"vlm_is_bus_stop\": False,\n","        \"vlm_conf\": 0.0,\n","        \"vlm_reason\": \"\",\n","        \"vote_is_bus_stop\": False,\n","        \"vote_conf_mean\": None,\n","        \"vote_raw\": None,\n","        \"clf_p\": None,\n","        \"final_label\": None,\n","        \"final_reason\": \"\",\n","    }\n","    try:\n","        img = Image.open(img_path).convert('RGB')\n","        # 1) obtener bbox\n","        bbox = None\n","        if meta and meta.bbox_xywh is not None:\n","            bbox = meta.bbox_xywh\n","        else:\n","            bbox = detect_red_box_bbox(img)\n","        if bbox is None:\n","            out[\"status\"] = \"no_bbox\"\n","            out[\"final_label\"] = False\n","            out[\"final_reason\"] = \"No se detectó rectángulo rojo ni bbox meta\"\n","            return out\n","        out[\"has_bbox\"] = True\n","        crop = crop_from_bbox(img, bbox, pad=0.3)\n","\n","        # 2) Gate geo (si disponible)\n","        lat_c = None\n","        lon_c = None\n","        zoom = DEFAULT_ZOOM\n","        if meta:\n","            if meta.center_lat is not None and meta.center_lon is not None:\n","                lat_c, lon_c = meta.center_lat, meta.center_lon\n","            if meta.zoom is not None:\n","                zoom = meta.zoom\n","        if (lat_c is None or lon_c is None):\n","            # intento parsear del nombre\n","            lat_c, lon_c = parse_latlon_from_name(os.path.basename(img_path))\n","        if USE_GEO_GATE and gpd is not None and lat_c is not None and lon_c is not None and roads_gdf is not None:\n","            # centro del bbox (px) a lat/lon\n","            cx, cy, w, h = bbox\n","            lat_px, lon_px = pixel_to_latlon(lat_c, lon_c, zoom, TILE_SIZE, cx, cy)\n","            out[\"geo_gate_used\"] = True\n","            d = dist_to_nearest_road_m(lat_px, lon_px, roads_gdf)\n","            out[\"dist_to_road_m\"] = round(d, 2)\n","            if d > MAX_DIST_TO_ROAD_M:\n","                out[\"geo_ok\"] = False\n","            # Buildings (opcional)\n","            if USE_BUILDINGS_GATE and buildings_gdf is not None:\n","                inter = intersects_building(lat_px, lon_px, buildings_gdf)\n","                out[\"building_intersection\"] = bool(inter)\n","                if inter:\n","                    out[\"geo_ok\"] = False\n","\n","        if not out[\"geo_ok\"]:\n","            out[\"final_label\"] = False\n","            out[\"final_reason\"] = f\"Geo gate: distancia a ruta > {MAX_DIST_TO_ROAD_M} m o en edificio\"\n","            return out\n","\n","        # 3) VLM (FULL + CROP)\n","        r0 = vlm_verify_full_and_crop(img, crop, model, processor)\n","        out[\"vlm_is_bus_stop\"] = bool(r0.get(\"is_bus_stop\", False))\n","        out[\"vlm_conf\"] = float(r0.get(\"confidence\", 0.0))\n","        out[\"vlm_reason\"] = str(r0.get(\"reason\", \"\"))\n","\n","        # 4) Consenso por rotaciones (solo CROP)\n","        votes = []\n","        for ang in ROTATIONS:\n","            if ang == 0:\n","                crop_r = crop\n","            else:\n","                crop_r = crop.rotate(ang, expand=True)\n","            ri = vlm_verify_crop_only(crop_r, model, processor)\n","            votes.append(float(ri.get(\"confidence\", 0.0)) * (1.0 if ri.get(\"is_bus_stop\", False) else -1.0))\n","        conf_mean = float(np.mean([abs(v) for v in votes])) if votes else None\n","        is_true_majority = sum(1 for v in votes if v > 0) > len(votes) / 2.0\n","        out[\"vote_conf_mean\"] = None if conf_mean is None else round(conf_mean, 3)\n","        out[\"vote_is_bus_stop\"] = bool(is_true_majority and (conf_mean is not None and conf_mean >= VOTE_THR))\n","        out[\"vote_raw\"] = \",\".join(f\"{v:.3f}\" for v in votes)\n","\n","        # 5) Clasificador ligero (opcional)\n","        p_clf = None\n","        if clf is not None and siglip_mdl is not None and siglip_proc is not None:\n","            p_clf = clf_score_crop(crop, clf, siglip_mdl, siglip_proc, device)\n","            out[\"clf_p\"] = round(float(p_clf), 3)\n","\n","        # 6) Decisión final (reglas simples, ajustables)\n","        reasons = []\n","        label = False\n","\n","        # base: usar consenso si existe\n","        if out[\"vote_is_bus_stop\"]:\n","            label = True\n","            reasons.append(f\"consenso_rotaciones_conf≈{out['vote_conf_mean']}\")\n","        else:\n","            # fallback a decisión VLM base si es muy segura\n","            if out[\"vlm_is_bus_stop\"] and out[\"vlm_conf\"] >= 0.75:\n","                label = True\n","                reasons.append(f\"vlm_full+crop_conf={out['vlm_conf']:.2f}\")\n","\n","        # influenciar con clasificador ligero\n","        if p_clf is not None:\n","            if p_clf >= 0.80:\n","                label = True\n","                reasons.append(f\"clf_siglip_p={p_clf:.2f}\")\n","            elif p_clf <= 0.20:\n","                label = False\n","                reasons.append(f\"clf_siglip_p={p_clf:.2f}\")\n","\n","        # zonas grises: forzar revisión manual\n","        if not label and out[\"vlm_conf\"] >= UNCERTAIN_RANGE[0] and out[\"vlm_conf\"] <= UNCERTAIN_RANGE[1]:\n","            reasons.append(\"incertidumbre_VLM\")\n","\n","        out[\"final_label\"] = bool(label)\n","        out[\"final_reason\"] = \"; \".join([out[\"vlm_reason\"]] + reasons if out[\"vlm_reason\"] else reasons)\n","        return out\n","\n","    except Exception as e:\n","        out[\"status\"] = f\"error:{type(e).__name__}:{e}\"\n","        out[\"final_label\"] = False\n","        out[\"final_reason\"] = \"exception\"\n","        return out\n","\n","\n","def run_pipeline(\n","    images_dir: str,\n","    output_csv: str,\n","    model,\n","    processor,\n","    use_meta_bbox: bool = USE_META_BBOX,\n","    path_bbox_meta: Optional[str] = PATH_BBOX_META,\n","    use_geo_gate: bool = USE_GEO_GATE,\n","    path_roads: Optional[str] = PATH_ROADS,\n","    use_buildings_gate: bool = USE_BUILDINGS_GATE,\n","    path_buildings: Optional[str] = PATH_BUILDINGS,\n","    clf_path: Optional[str] = None,          # ruta a joblib del clasificador ligero (opcional)\n","    siglip_device: str = \"cuda\",\n","    recursive: bool = False                  # NUEVO: buscar imágenes en subcarpetas\n","):\n","    # cargar meta\n","    meta_map: Dict[str, ImageMeta] = {}\n","    if use_meta_bbox and path_bbox_meta and os.path.isfile(path_bbox_meta):\n","        meta_map = read_meta_csv(path_bbox_meta)\n","\n","    # geo\n","    roads = None\n","    buildings = None\n","    if use_geo_gate and path_roads and os.path.isfile(path_roads):\n","        roads = load_roads(path_roads)\n","    if use_geo_gate and use_buildings_gate and path_buildings and os.path.isfile(path_buildings):\n","        buildings = load_buildings(path_buildings)\n","\n","    # clasificador ligero\n","    clf = None\n","    siglip_mdl = None\n","    siglip_proc = None\n","    if clf_path and os.path.isfile(clf_path):\n","        clf = load_light_classifier(clf_path)\n","        siglip_mdl, siglip_proc = load_siglip_model(siglip_device)\n","\n","    # inicializar filas ANTES del loop (evita UnboundLocalError)\n","    rows = []\n","\n","    if not os.path.isdir(images_dir):\n","        raise FileNotFoundError(f\"images_dir no existe: {images_dir}\")\n","\n","    file_paths: List[str] = []\n","    if recursive:\n","        for root, _, fnames in os.walk(images_dir):\n","            for fn in fnames:\n","                if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    file_paths.append(os.path.join(root, fn))\n","    else:\n","        file_paths = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n","\n","    file_paths.sort()\n","    if not file_paths:\n","        raise FileNotFoundError(f\"No se encontraron imágenes (*.png|*.jpg|*.jpeg) en {images_dir}\")\n","\n","    for i, p in enumerate(file_paths, 1):\n","        fn = os.path.basename(p)\n","        mm = meta_map.get(fn)\n","        res = verify_image(p, model, processor, roads, buildings, mm, clf, siglip_mdl, siglip_proc, siglip_device)\n","        rows.append(res)\n","        if i % 25 == 0:\n","            print(f\"Procesadas {i}/{len(file_paths)} imágenes...\")\n","\n","    # guardar CSV\n","    fieldnames = list(rows[0].keys()) if rows else []\n","    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n","    with open(output_csv, \"w\", newline='', encoding='utf-8') as f:\n","        writer = csv.DictWriter(f, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for r in rows:\n","            writer.writerow(r)\n","    print(f\"Listo. Guardado CSV en: {output_csv}\")\n","\n","\n","# ======================\n","# Post-proceso: unir CSV + imágenes, bucketing y galería (opcional)\n","# ======================\n","\n","def _collect_image_index(images_dir: str) -> Dict[str, str]:\n","    \"\"\"Índice {basename -> ruta completa} (recursivo). Si hay duplicados, se elige el path más corto.\"\"\"\n","    idx: Dict[str, str] = {}\n","    for root, _, files in os.walk(images_dir):\n","        for fn in files:\n","            if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                p = os.path.join(root, fn)\n","                b = os.path.basename(fn)\n","                if b not in idx or len(p) < len(idx[b]):\n","                    idx[b] = p\n","    return idx\n","\n","\n","def join_csv_with_images(results_csv: str, images_dir: str):\n","    \"\"\"Devuelve DataFrame con columnas del CSV + `image_path` y `exists`.\n","    Requiere `pandas`.\n","    \"\"\"\n","    if pd is None:\n","        raise RuntimeError(\"Pandas no disponible. Instala: pip install pandas\")\n","    if not os.path.isfile(results_csv):\n","        raise FileNotFoundError(f\"No existe CSV: {results_csv}\")\n","    if not os.path.isdir(images_dir):\n","        raise FileNotFoundError(f\"No existe carpeta de imágenes: {images_dir}\")\n","\n","    df = pd.read_csv(results_csv)\n","    if \"filename\" not in df.columns:\n","        raise ValueError(\"El CSV debe tener columna 'filename'.\")\n","\n","    idx = _collect_image_index(images_dir)\n","    df[\"image_path\"] = df[\"filename\"].apply(lambda x: idx.get(os.path.basename(str(x))))\n","    df[\"exists\"] = df[\"image_path\"].apply(lambda p: bool(isinstance(p, str) and os.path.isfile(p)))\n","    return df\n","\n","\n","def bucketize_for_review(df, out_base: str, uncertain_range: Tuple[float, float] = (0.35, 0.60)) -> Dict[str, int]:\n","    \"\"\"Copia imágenes en out_base/true, out_base/false, out_base/review según reglas de incertidumbre.\"\"\"\n","    os.makedirs(out_base, exist_ok=True)\n","    out_true = os.path.join(out_base, \"true\"); os.makedirs(out_true, exist_ok=True)\n","    out_false= os.path.join(out_base, \"false\");os.makedirs(out_false, exist_ok=True)\n","    out_rev  = os.path.join(out_base, \"review\");os.makedirs(out_rev, exist_ok=True)\n","\n","    import shutil\n","    lo, hi = uncertain_range\n","    stats = {\"true\":0, \"false\":0, \"review\":0}\n","\n","    for _, r in df.iterrows():\n","        p = r.get(\"image_path\")\n","        if not (isinstance(p, str) and os.path.isfile(p)):\n","            continue\n","        b = os.path.basename(p)\n","        status = str(r.get(\"status\", \"\"))\n","        has_bbox = bool(r.get(\"has_bbox\", False))\n","        final_label = bool(r.get(\"final_label\", False))\n","        vlm_conf = float(r.get(\"vlm_conf\", 0.0))\n","        vote_conf = r.get(\"vote_conf_mean\")\n","        vote_conf = float(vote_conf) if (pd is not None and pd.notna(vote_conf)) else (vote_conf if isinstance(vote_conf, float) else None)\n","\n","        uncertain = (status != \"ok\") or (not has_bbox) or (lo <= vlm_conf <= hi) or (vote_conf is not None and lo <= vote_conf <= hi)\n","        dst = out_rev if uncertain else (out_true if final_label else out_false)\n","        try:\n","            shutil.copy2(p, os.path.join(dst, b))\n","            key = \"review\" if dst==out_rev else (\"true\" if dst==out_true else \"false\")\n","            stats[key] += 1\n","        except Exception:\n","            pass\n","    return stats\n","\n","\n","def build_html_gallery(df, out_dir: str, copy_images: bool = True, cols: int = 4, max_width: int = 420) -> str:\n","    \"\"\"Genera una galería HTML de validación visual. Retorna la ruta al index.html.\"\"\"\n","    os.makedirs(out_dir, exist_ok=True)\n","    imgs_dir = os.path.join(out_dir, \"imgs\")\n","    if copy_images:\n","        os.makedirs(imgs_dir, exist_ok=True)\n","    import shutil\n","\n","    rows = []\n","    for _, r in df.iterrows():\n","        p = r.get(\"image_path\")\n","        if not (isinstance(p, str) and os.path.isfile(p)):\n","            continue\n","        b = os.path.basename(p)\n","        label = bool(r.get(\"final_label\", False))\n","        vlm_conf = float(r.get(\"vlm_conf\", 0.0))\n","        vote_conf = r.get(\"vote_conf_mean\")\n","        vote_conf = float(vote_conf) if (pd is not None and pd.notna(vote_conf)) else None\n","        reason = str(r.get(\"final_reason\", \"\"))[:120]\n","\n","        if copy_images:\n","            dst_rel = f\"imgs/{b}\"\n","            try:\n","                shutil.copy2(p, os.path.join(imgs_dir, b))\n","            except Exception:\n","                dst_rel = p\n","        else:\n","            dst_rel = p\n","        border = \"#2e7d32\" if label else \"#c62828\"\n","        rows.append((dst_rel, border, label, vlm_conf, vote_conf, reason, b))\n","\n","    html_path = os.path.join(out_dir, \"index.html\")\n","    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"\"\"<!doctype html>\n","<html lang=\\\"es\\\"><head><meta charset=\\\"utf-8\\\"/>\n","<title>Revisión de paradas</title>\n","<style>\n","body{{font-family:system-ui,Segoe UI,Arial,sans-serif;background:#fafafa;margin:16px}}\n",".grid{{display:grid;grid-template-columns:repeat({cols},1fr);gap:14px}}\n",".card{{background:white;border-radius:12px;box-shadow:0 2px 8px rgba(0,0,0,.08);padding:10px}}\n",".card img{{width:100%;height:auto;max-width:{max_width}px;border-radius:10px;border:4px solid var(--b)}}\n",".meta{{font-size:12px;color:#333;margin-top:6px;white-space:pre-wrap}}\n",".badge{{display:inline-block;padding:2px 8px;border-radius:999px;background:#eee;margin-right:6px;font-size:12px}}\n",".true{{background:#e8f5e9}} .false{{background:#ffebee}}\n","</style></head><body>\n","<h1>Revisión de paradas</h1>\n","<div class=\\\"badge true\\\">Verde=True</div><div class=\\\"badge false\\\">Rojo=False</div>\n","<div class=\\\"grid\\\">\"\"\")\n","        for dst_rel, border, label, vlm_conf, vote_conf, reason, b in rows:\n","            f.write(f\"\"\"\n","<div class=\\\"card\\\" style=\\\"--b:{border}\\\">\n","  <img src=\\\"{dst_rel}\\\" alt=\\\"{b}\\\">\n","  <div class=\\\"meta\\\">\n","    <b>{'TRUE' if label else 'FALSE'}</b> · vlm={vlm_conf:.2f} · vote={'' if vote_conf is None else f'{vote_conf:.2f}'}<br>\n","    <b>file:</b> {b}<br>\n","    <b>reason:</b> {reason}\n","  </div>\n","</div>\"\"\")\n","        f.write(\"\"\"\n","</div></body></html>\"\"\")\n","    return html_path\n","\n","\n","def posthoc_recalibrate_and_bucket(results_csv: str, images_dir: str, out_dir: str,\n","                                   vote_thr: float = 0.45, vlm_thr: float = 0.60,\n","                                   uncertain_range: Tuple[float, float] = (0.30, 0.55),\n","                                   make_gallery: bool = False) -> Dict[str, Any]:\n","    \"\"\"Reclasifica sin re-inferir (umbrales más permisivos) y copia a true/false/review. Opcional: galería HTML.\"\"\"\n","    if pd is None:\n","        raise RuntimeError(\"Pandas no disponible. Instala: pip install pandas\")\n","    df = join_csv_with_images(results_csv, images_dir)\n","    df = df[df[\"exists\"]]\n","\n","    # nueva etiqueta\n","    new_true = []\n","    for _, r in df.iterrows():\n","        vlm_ok = bool(r.get(\"vlm_is_bus_stop\", False))\n","        vlm_conf = float(r.get(\"vlm_conf\", 0.0))\n","        vote_ok = bool(r.get(\"vote_is_bus_stop\", False))\n","        vote_conf = r.get(\"vote_conf_mean\")\n","        vote_conf = float(vote_conf) if pd.notna(vote_conf) else 0.0\n","        is_true = (vote_ok and vote_conf >= vote_thr) or (vlm_ok and vlm_conf >= vlm_thr)\n","        new_true.append(is_true)\n","    df[\"new_true\"] = new_true\n","\n","    stats = bucketize_for_review(df, out_dir, uncertain_range=uncertain_range)\n","\n","    out = {\"counts\": stats}\n","    if make_gallery:\n","        html = build_html_gallery(df, os.path.join(out_dir, \"galeria\"), copy_images=True)\n","        out[\"gallery\"] = html\n","    return out\n","\n","\n","# ======================\n","# Debug de crops (exportar FULL+BOX y CROP)\n","# ======================\n","\n","def _draw_box(img: Image.Image, bbox_xywh: Tuple[int,int,int,int], color=(255,0,0), width: int = 3) -> Image.Image:\n","    img2 = img.copy()\n","    draw = ImageDraw.Draw(img2)\n","    cx, cy, w, h = bbox_xywh\n","    x0, y0 = cx - w//2, cy - h//2\n","    x1, y1 = cx + w//2, cy + h//2\n","    for i in range(width):\n","        draw.rectangle([x0-i, y0-i, x1+i, y1+i], outline=color)\n","    return img2\n","\n","\n","def export_debug_crops(images_dir: str, out_dir: str, sample_n: int = 30,\n","                       r_thr: int = 170, gb_thr: int = 120, pad: float = 0.3) -> Dict[str,int]:\n","    os.makedirs(out_dir, exist_ok=True)\n","    paths: List[str] = []\n","    for root, _, files in os.walk(images_dir):\n","        for fn in files:\n","            if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                paths.append(os.path.join(root, fn))\n","    paths.sort()\n","    if sample_n and len(paths) > sample_n:\n","        import random\n","        paths = random.sample(paths, sample_n)\n","\n","    ok = miss = 0\n","    for p in paths:\n","        img = Image.open(p).convert(\"RGB\")\n","        bbox = detect_red_box_bbox(img, r_thr=r_thr, gb_thr=gb_thr)\n","        if bbox is None:\n","            miss += 1\n","            img.save(os.path.join(out_dir, f\"MISS_{os.path.basename(p)}\"))\n","            continue\n","        ok += 1\n","        crop = crop_from_bbox(img, bbox, pad=pad)\n","        with_box = _draw_box(img, bbox)\n","        with_box.save(os.path.join(out_dir, f\"FULL_{os.path.basename(p)}\"))\n","        crop.save(os.path.join(out_dir, f\"CROP_{os.path.basename(p)}\"))\n","    return {\"ok\": ok, \"miss\": miss}\n","\n","\n","# ======================\n","# Ejemplo de uso (referencia)\n","# ======================\n","\"\"\"\n","# Requisitos (ejecutar una vez por sesión):\n","# Opción estable:\n","# !pip -q install -U \"transformers>=4.44.2\" \"accelerate>=0.33.0\" \"qwen-vl-utils>=0.0.8\" \"bitsandbytes>=0.43.0\"\n","# Si ves errores de 'qwen2_vl' o de configuración, instala desde fuente (más reciente):\n","# !pip -q install -U git+https://github.com/huggingface/transformers\n","# Reiniciar el runtime si te lo pide.\n","\n","from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n","import torch\n","\n","model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n","\n","# Carga recomendada para T4 (FP16). Para errores de mapeo ('qwen2_vl' / config), usa la última versión de Transformers\n","# o instala desde fuente: pip install -U git+https://github.com/huggingface/transformers\n","processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.float16,         # T4 no soporta bfloat16\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","# Ahora correr el pipeline\n","run_pipeline(\n","    images_dir=IMAGES_DIR,\n","    output_csv=OUTPUT_CSV,\n","    model=model,\n","    processor=processor,\n","    use_meta_bbox=USE_META_BBOX,\n","    path_bbox_meta=PATH_BBOX_META,\n","    use_geo_gate=USE_GEO_GATE,\n","    path_roads=PATH_ROADS,\n","    use_buildings_gate=USE_BUILDINGS_GATE,\n","    path_buildings=PATH_BUILDINGS,\n","    clf_path=None,   # o \".../clf_siglip.joblib\" si ya entrenaste\n","    siglip_device=\"cuda\",\n",")\n","\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"B-rfF7AkBIsN","executionInfo":{"status":"ok","timestamp":1755370023813,"user_tz":180,"elapsed":94,"user":{"displayName":"Ana Araújo","userId":"14657118052263951842"}},"outputId":"e21ccad3-6f5c-4f30-acc3-f0a9ef8333b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Requisitos (ejecutar una vez por sesión):\\n# Opción estable:\\n# !pip -q install -U \"transformers>=4.44.2\" \"accelerate>=0.33.0\" \"qwen-vl-utils>=0.0.8\" \"bitsandbytes>=0.43.0\"\\n# Si ves errores de \\'qwen2_vl\\' o de configuración, instala desde fuente (más reciente):\\n# !pip -q install -U git+https://github.com/huggingface/transformers\\n# Reiniciar el runtime si te lo pide.\\n\\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\\nimport torch\\n\\nmodel_name = \"Qwen/Qwen2-VL-7B-Instruct\"\\n\\n# Carga recomendada para T4 (FP16). Para errores de mapeo (\\'qwen2_vl\\' / config), usa la última versión de Transformers\\n# o instala desde fuente: pip install -U git+https://github.com/huggingface/transformers\\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\\n    model_name,\\n    torch_dtype=torch.float16,         # T4 no soporta bfloat16\\n    device_map=\"auto\",\\n    trust_remote_code=True,\\n)\\n\\n# Ahora correr el pipeline\\nrun_pipeline(\\n    images_dir=IMAGES_DIR,\\n    output_csv=OUTPUT_CSV,\\n    model=model,\\n    processor=processor,\\n    use_meta_bbox=USE_META_BBOX,\\n    path_bbox_meta=PATH_BBOX_META,\\n    use_geo_gate=USE_GEO_GATE,\\n    path_roads=PATH_ROADS,\\n    use_buildings_gate=USE_BUILDINGS_GATE,\\n    path_buildings=PATH_BUILDINGS,\\n    clf_path=None,   # o \".../clf_siglip.joblib\" si ya entrenaste\\n    siglip_device=\"cuda\",\\n)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Requisitos (ejecutar una vez por sesión):\n","# Opción estable:\n","# !pip -q install -U \"transformers>=4.44.2\" \"accelerate>=0.33.0\" \"qwen-vl-utils>=0.0.8\" \"bitsandbytes>=0.43.0\"\n","# Si ves errores de 'qwen2_vl' o de configuración, instala desde fuente (más reciente):\n","# !pip -q install -U git+https://github.com/huggingface/transformers\n","# Reiniciar el runtime si te lo pide.\n","\n","from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n","import torch\n","\n","model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n","\n","# Carga recomendada para T4 (FP16). Para errores de mapeo ('qwen2_vl' / config), usa la última versión de Transformers\n","# o instala desde fuente: pip install -U git+https://github.com/huggingface/transformers\n","processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.float16,         # T4 no soporta bfloat16\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["39e960e112c447958cf521cfded3d3a7","d55e916bd70d47a2b803c6227646ff52","6e920ac841784758ad53dec33c017c50","04bf7639c0794db7a4330ddd92251afd","3f2ee1de492d4aac95203f1d9b94d9f0","0c31144f141044078afc07f5f2ec2f8c","80d445af2522413cb44d0478aa25dca0","fdb073120c3c49f7aadb63e4984f7c7d","9233022d40404dafa12a6b370c107545","1f3f27f5c4f04e318ff30228878c3d50","bda899e211364495861866500bc4a2a8"]},"id":"lUZ8_SHd-Ai1","executionInfo":{"status":"ok","timestamp":1755368315703,"user_tz":180,"elapsed":166570,"user":{"displayName":"Ana Araújo","userId":"14657118052263951842"}},"outputId":"50343dc6-f077-4bd5-ad3f-6746cdce0641"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e960e112c447958cf521cfded3d3a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 25/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 50/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 75/754 imágenes...\n","Procesadas 100/754 imágenes...\n","Procesadas 125/754 imágenes...\n","Procesadas 150/754 imágenes...\n","Procesadas 175/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 200/754 imágenes...\n","Procesadas 225/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 250/754 imágenes...\n","Procesadas 275/754 imágenes...\n","Procesadas 300/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 325/754 imágenes...\n","Procesadas 350/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 375/754 imágenes...\n","Procesadas 400/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 425/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 450/754 imágenes...\n","Procesadas 475/754 imágenes...\n","Procesadas 500/754 imágenes...\n","Procesadas 525/754 imágenes...\n","Procesadas 550/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 575/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 600/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 625/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 650/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 675/754 imágenes...\n","Procesadas 700/754 imágenes...\n","Procesadas 725/754 imágenes...\n","Procesadas 750/754 imágenes...\n","Listo. Guardado CSV en: /content/drive/My Drive/Tesis/Resultados/verificacion_paradas_v1.csv\n"]}]},{"cell_type":"code","source":["run_pipeline(\n","    images_dir=IMAGES_DIR,\n","    output_csv=OUTPUT_CSV,\n","    model=model,\n","    processor=processor,\n","    recursive=True,   # ponelo en True si tenés subcarpetas\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwNkPPbA_yuA","executionInfo":{"status":"ok","timestamp":1755370064415,"user_tz":180,"elapsed":17555,"user":{"displayName":"Ana Araújo","userId":"14657118052263951842"}},"outputId":"639f2553-1ca9-48ae-9e5f-73a04abf3928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 25/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 50/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 75/754 imágenes...\n","Procesadas 100/754 imágenes...\n","Procesadas 125/754 imágenes...\n","Procesadas 150/754 imágenes...\n","Procesadas 175/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 200/754 imágenes...\n","Procesadas 225/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 250/754 imágenes...\n","Procesadas 275/754 imágenes...\n","Procesadas 300/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 325/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 350/754 imágenes...\n","Procesadas 375/754 imágenes...\n","Procesadas 400/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 425/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 450/754 imágenes...\n","Procesadas 475/754 imágenes...\n","Procesadas 500/754 imágenes...\n","Procesadas 525/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 550/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 575/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 600/754 imágenes...\n","Procesadas 625/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 650/754 imágenes...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Procesadas 675/754 imágenes...\n","Procesadas 700/754 imágenes...\n","Procesadas 725/754 imágenes...\n","Procesadas 750/754 imágenes...\n","Listo. Guardado CSV en: /content/drive/My Drive/Tesis/Resultados/verificacion_paradas_v1.csv\n"]}]},{"cell_type":"code","source":["# === Bucketing de imágenes en true/false/review (+ opcional: recalibración y galería) ===\n","import os, re, shutil\n","import pandas as pd\n","\n","# ---------- CONFIG (ajustar) ----------\n","RESULTS_CSV = \"/content/drive/My Drive/Tesis/Resultados/verificacion_paradas_v1.csv\"\n","IMAGES_DIR  = \"/content/drive/My Drive/Tesis/Datos/Ruta8/paradas_no_seguras\"\n","OUT_DIR     = \"/content/drive/My Drive/Tesis/Resultados/revision_paradas_v1\"\n","UNCERTAIN_RANGE = (0.35, 0.60)  # casos con conf intermedia => review\n","\n","# Recalibración (más permisiva). Si False, usa las etiquetas del CSV tal cual.\n","USE_RECALIBRATION = True\n","VOTE_THR   = 0.45   # si hay vote_is_bus_stop y vote_conf_mean >= VOTE_THR\n","VLM_THR    = 0.60   # o si vlm_is_bus_stop y vlm_conf >= VLM_THR\n","\n","# Galería HTML (opcional)\n","MAKE_GALLERY = True\n","GALLERY_DIR  = os.path.join(OUT_DIR, \"galeria\")\n","GALLERY_COLS = 4\n","GALLERY_MAXW = 420\n","# -------------------------------------\n","\n","os.makedirs(OUT_DIR, exist_ok=True)\n","OUT_TRUE  = os.path.join(OUT_DIR, \"true\");  os.makedirs(OUT_TRUE, exist_ok=True)\n","OUT_FALSE = os.path.join(OUT_DIR, \"false\"); os.makedirs(OUT_FALSE, exist_ok=True)\n","OUT_REV   = os.path.join(OUT_DIR, \"review\");os.makedirs(OUT_REV, exist_ok=True)\n","\n","def collect_image_index(images_dir: str):\n","    \"\"\"Indexa recursivamente {basename -> ruta completa}. Si hay duplicados, elige el path más corto.\"\"\"\n","    idx, dups = {}, {}\n","    for root, _, files in os.walk(images_dir):\n","        for fn in files:\n","            if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                p = os.path.join(root, fn)\n","                b = os.path.basename(fn)\n","                if b in idx:\n","                    dups.setdefault(b, []).append(p)\n","                    if len(p) < len(idx[b]):  # heurística simple\n","                        idx[b] = p\n","                else:\n","                    idx[b] = p\n","    if dups:\n","        print(f\"Aviso: {len(dups)} nombres de archivo duplicados; se eligió un path por nombre.\")\n","    return idx\n","\n","def sanitize(s: str, maxlen=120) -> str:\n","    s = re.sub(r'[\\\\/:*?\"<>|]', \"_\", str(s))\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s[:maxlen]\n","\n","# 1) Cargar CSV y mapear imágenes\n","if not os.path.isfile(RESULTS_CSV):\n","    raise FileNotFoundError(f\"No existe CSV: {RESULTS_CSV}\")\n","if not os.path.isdir(IMAGES_DIR):\n","    raise FileNotFoundError(f\"No existe carpeta de imágenes: {IMAGES_DIR}\")\n","\n","df = pd.read_csv(RESULTS_CSV)\n","if \"filename\" not in df.columns:\n","    raise ValueError(\"El CSV debe tener columna 'filename' (nombre del archivo).\")\n","\n","idx = collect_image_index(IMAGES_DIR)\n","df[\"image_path\"] = df[\"filename\"].apply(lambda x: idx.get(os.path.basename(str(x))))\n","df[\"exists\"] = df[\"image_path\"].apply(lambda p: bool(isinstance(p, str) and os.path.isfile(p)))\n","df_ex = df[df[\"exists\"]].copy()\n","\n","print(\"Total filas CSV:\", len(df))\n","print(\"Con imagen encontrada:\", len(df_ex))\n","print(\"Sin imagen:\", len(df) - len(df_ex))\n","\n","# 2) Decidir etiqueta final (con o sin recalibración)\n","def decide_label(row):\n","    if not USE_RECALIBRATION:\n","        return bool(row.get(\"final_label\", False))\n","    # Recalibración más permisiva\n","    vlm_ok   = bool(row.get(\"vlm_is_bus_stop\", False))\n","    vlm_conf = float(row.get(\"vlm_conf\", 0.0))\n","    vote_ok  = bool(row.get(\"vote_is_bus_stop\", False))\n","    vote_cm  = row.get(\"vote_conf_mean\")\n","    vote_cm  = float(vote_cm) if pd.notna(vote_cm) else 0.0\n","    return (vote_ok and vote_cm >= VOTE_THR) or (vlm_ok and vlm_conf >= VLM_THR)\n","\n","df_ex[\"label_new\"] = df_ex.apply(decide_label, axis=1)\n","\n","# 3) Enviar a true / false / review según incertidumbre\n","lo, hi = UNCERTAIN_RANGE\n","cop = {\"true\":0, \"false\":0, \"review\":0, \"omitidas\":0}\n","\n","for _, r in df_ex.iterrows():\n","    p = r[\"image_path\"]\n","    b = os.path.basename(p)\n","\n","    status = str(r.get(\"status\", \"\"))\n","    has_bbox = bool(r.get(\"has_bbox\", False))\n","    vlm_conf = float(r.get(\"vlm_conf\", 0.0))\n","    vote_conf = r.get(\"vote_conf_mean\")\n","    vote_conf = float(vote_conf) if pd.notna(vote_conf) else None\n","\n","    # incierto si: error, sin bbox, o conf intermedia\n","    uncertain = (\n","        status != \"ok\" or (not has_bbox) or\n","        (lo <= vlm_conf <= hi) or\n","        (vote_conf is not None and lo <= vote_conf <= hi)\n","    )\n","\n","    lbl = bool(r[\"label_new\"])\n","    dst = OUT_REV if uncertain else (OUT_TRUE if lbl else OUT_FALSE)\n","\n","    name_out = f\"{'T' if lbl else 'F'}_vlm{vlm_conf:.2f}{'' if vote_conf is None else f'_vote{vote_conf:.2f}'}_{b}\"\n","    try:\n","        shutil.copy2(p, os.path.join(dst, name_out))\n","        key = \"review\" if dst==OUT_REV else (\"true\" if dst==OUT_TRUE else \"false\")\n","        cop[key] += 1\n","    except Exception as e:\n","        cop[\"omitidas\"] += 1\n","        print(\"No pude copiar:\", p, \"->\", e)\n","\n","print(\"\\nCopias:\", cop)\n","print(\"Carpetas listas:\\n\", OUT_TRUE, \"\\n\", OUT_FALSE, \"\\n\", OUT_REV)\n","\n","# 4) (Opcional) Galería HTML con tarjetas\n","if MAKE_GALLERY:\n","    os.makedirs(GALLERY_DIR, exist_ok=True)\n","    imgs_dir = os.path.join(GALLERY_DIR, \"imgs\"); os.makedirs(imgs_dir, exist_ok=True)\n","\n","    rows = []\n","    for _, r in df_ex.iterrows():\n","        p = r[\"image_path\"]\n","        if not os.path.isfile(p):\n","            continue\n","        b = os.path.basename(p)\n","        lbl = bool(r[\"label_new\"])\n","        vlm_conf = float(r.get(\"vlm_conf\", 0.0))\n","        vote_conf = r.get(\"vote_conf_mean\")\n","        vote_conf = float(vote_conf) if pd.notna(vote_conf) else None\n","        reason = sanitize(r.get(\"final_reason\", \"\"))\n","\n","        dst_rel = f\"imgs/{b}\"\n","        try:\n","            shutil.copy2(p, os.path.join(imgs_dir, b))\n","        except Exception:\n","            dst_rel = p\n","\n","        border = \"#2e7d32\" if lbl else \"#c62828\"\n","        rows.append((dst_rel, border, lbl, vlm_conf, vote_conf, reason, b))\n","\n","    html = os.path.join(GALLERY_DIR, \"index.html\")\n","    with open(html, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"\"\"<!doctype html>\n","<html lang=\"es\"><head><meta charset=\"utf-8\"/>\n","<title>Revisión de paradas</title>\n","<style>\n","body{{font-family:system-ui,Segoe UI,Arial,sans-serif;background:#fafafa;margin:16px}}\n",".grid{{display:grid;grid-template-columns:repeat({GALLERY_COLS},1fr);gap:14px}}\n",".card{{background:white;border-radius:12px;box-shadow:0 2px 8px rgba(0,0,0,.08);padding:10px}}\n",".card img{{width:100%;height:auto;max-width:{GALLERY_MAXW}px;border-radius:10px;border:4px solid var(--b)}}\n",".meta{{font-size:12px;color:#333;margin-top:6px;white-space:pre-wrap}}\n",".badge{{display:inline-block;padding:2px 8px;border-radius:999px;background:#eee;margin-right:6px;font-size:12px}}\n",".true{{background:#e8f5e9}} .false{{background:#ffebee}}\n","</style></head><body>\n","<h1>Revisión de paradas</h1>\n","<div class=\"badge true\">Verde=True</div> <div class=\"badge false\">Rojo=False</div>\n","<div class=\"grid\">\"\"\")\n","        for dst_rel, border, lbl, vc, votec, reason, b in rows:\n","            f.write(f\"\"\"\n","<div class=\"card\" style=\"--b:{border}\">\n","  <img src=\"{dst_rel}\" alt=\"{b}\">\n","  <div class=\"meta\">\n","    <b>{'TRUE' if lbl else 'FALSE'}</b> · vlm={vc:.2f} · vote={'' if votec is None else f'{votec:.2f}'}<br>\n","    <b>file:</b> {b}<br>\n","    <b>reason:</b> {reason}\n","  </div>\n","</div>\"\"\")\n","        f.write(\"\"\"\n","</div></body></html>\"\"\")\n","    print(\"Galería:\", html)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZ6dUywEOsWq","executionInfo":{"status":"ok","timestamp":1755370557303,"user_tz":180,"elapsed":27585,"user":{"displayName":"Ana Araújo","userId":"14657118052263951842"}},"outputId":"e7baf7b4-3d6e-4722-a5ff-4a49fa1e3cc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total filas CSV: 754\n","Con imagen encontrada: 754\n","Sin imagen: 0\n","\n","Copias: {'true': 0, 'false': 0, 'review': 754, 'omitidas': 0}\n","Carpetas listas:\n"," /content/drive/My Drive/Tesis/Resultados/revision_paradas_v1/true \n"," /content/drive/My Drive/Tesis/Resultados/revision_paradas_v1/false \n"," /content/drive/My Drive/Tesis/Resultados/revision_paradas_v1/review\n","Galería: /content/drive/My Drive/Tesis/Resultados/revision_paradas_v1/galeria/index.html\n"]}]}]}